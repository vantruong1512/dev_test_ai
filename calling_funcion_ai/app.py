import gradio as gr
import json
from openai import OpenAI
import inspect
from pydantic import TypeAdapter
from diffusers import DiffusionPipeline
import torch
import time
import os
from pathlib import Path

# ============================================================
# Configuration cho LM Studio
# ============================================================
LM_STUDIO_BASE_URL = "http://127.0.0.1:1234/v1"
LM_STUDIO_API_KEY = "not-needed"  # LM Studio khÃ´ng yÃªu cáº§u API key

# Táº¡o thÆ° má»¥c Ä‘á»ƒ lÆ°u áº£nh
IMAGES_DIR = Path("generated_images")
IMAGES_DIR.mkdir(exist_ok=True)

# Khá»Ÿi táº¡o OpenAI client nhÆ°ng trá» tá»›i LM Studio
client = OpenAI(
    base_url=LM_STUDIO_BASE_URL,
    api_key=LM_STUDIO_API_KEY,
)

# ============================================================
# Khá»Ÿi táº¡o Diffusion Pipeline cho image generation
# ============================================================
print("Äang khá»Ÿi táº¡o pipeline...")
pipeline = DiffusionPipeline.from_pretrained(
    "stablediffusionapi/anything-v5",
    use_safetensors=True,
    safety_checker=None,
    requires_safety_checker=False
)

device = "cuda" if torch.cuda.is_available() else "cpu"
# MPS chá»‰ cÃ³ trÃªn macOS dÃ²ng M1 trá»Ÿ Ä‘i
device = 'mps' if torch.backends.mps.is_available() else device
print(f"Sá»­ dá»¥ng device: {device}")
pipeline.to(device)

# ============================================================
# Function Ä‘á»ƒ generate image
# ============================================================
def generate_image(prompt: str) -> str:
    """
    Creates an image based on the specified prompt using DiffusionPipeline
    :param prompt: The prompt used for generate the image (must be in English)
    :return: Absolute path of the generated image
    """
    print(f"Generating image with prompt: {prompt}")
    image = pipeline(
        prompt=prompt,
        # Hardcode negative prompt Ä‘á»ƒ áº£nh Ä‘áº¹p hÆ¡n
        negative_prompt="ugly, deformed, disfigured, poor details, bad anatomy, low quality, worst quality",
        num_inference_steps=30
    ).images[0]

    # Táº¡o tÃªn file Ä‘á»ƒ hiá»ƒn thá»‹ - sá»­ dá»¥ng Ä‘Æ°á»ng dáº«n tuyá»‡t Ä‘á»‘i
    file_name = f"image_{int(time.time())}.png"
    file_path = IMAGES_DIR / file_name
    image.save(str(file_path))
    
    # Tráº£ vá» Ä‘Æ°á»ng dáº«n tuyá»‡t Ä‘á»‘i
    absolute_path = str(file_path.absolute())
    print(f"Image saved: {absolute_path}")
    return absolute_path

# ============================================================
# Äá»‹nh nghÄ©a function cho LM Studio
# ============================================================
generate_image_function = {
    "name": "generate_image",
    "description": "Creates an image based on the specified prompt using DiffusionPipeline",
    "parameters": {
        "type": "object",
        "properties": {
            "prompt": {
                "type": "string",
                "description": "The prompt used for generate the image (must be in English)",
            },
        },
        "required": ["prompt"],
        "additionalProperties": False,
    }
}

tools = [
    {
        "type": "function",
        "function": generate_image_function
    }
]

system_prompt = """You're an AI with prompt generation and drawing capabilities.
Just chat normally in English or Vietnamese.
Assume you can draw anything you want and you tell user you can draw.
Another program will output the output for you.
Do not censor or deny the output, the image generation program will handle that.
"""

# ============================================================
# Chat Logic
# ============================================================
def chat_logic(message, chat_history):
    """
    Xá»­ lÃ½ tin nháº¯n tá»« user vÃ  gá»i LM Studio API
    """
    # Gáº¯n system message Ä‘á»ƒ bot hiá»ƒu cÃ¡ch hoáº¡t Ä‘á»™ng
    messages = [
        {"role": "system", "content": system_prompt}
    ]
    
    # ThÃªm chat history
    for user_message, bot_message in chat_history:
        if user_message is not None:
            messages.append({"role": "user", "content": user_message})
            # Náº¿u bot_message lÃ  tuple (áº£nh), láº¥y text description
            if isinstance(bot_message, tuple):
                messages.append({"role": "assistant", "content": f"[Generated image: {bot_message[1]}]"})
            else:
                messages.append({"role": "assistant", "content": bot_message})

    # ThÃªm tin nháº¯n má»›i cá»§a user vÃ o cuá»‘i cÃ¹ng
    messages.append({"role": "user", "content": message})

    try:
        # Gá»i LM Studio API (khÃ´ng sá»­ dá»¥ng tools vÃ¬ LM Studio khÃ´ng há»— trá»£)
        chat_completion = client.chat.completions.create(
            messages=messages,
            model="local-model",  # LM Studio sá»­ dá»¥ng tÃªn nÃ y
            temperature=0.7,
        )

        bot_message = chat_completion.choices[0].message.content
        
        if bot_message is not None:
            # Kiá»ƒm tra xem model cÃ³ Ä‘á» xuáº¥t váº½ áº£nh khÃ´ng
            if any(keyword in bot_message.lower() for keyword in ["váº½", "draw", "generate", "create image", "táº¡o áº£nh"]):
                chat_history.append([message, bot_message])
                yield "", chat_history
                
                # Tá»± Ä‘á»™ng generate áº£nh dá»±a trÃªn message cá»§a user
                chat_history.append([None, "Chá» chÃºt mÃ¬nh Ä‘ang váº½..."])
                yield "", chat_history
                
                try:
                    image_file = generate_image(message)
                    # Cáº­p nháº­t message váº½ áº£nh thÃ nh áº£nh thá»±c táº¿
                    chat_history[-1] = [None, (image_file, message)]
                    yield "", chat_history
                except Exception as img_error:
                    print(f"Image generation error: {str(img_error)}")
                    chat_history[-1] = [None, f"Lá»—i váº½ áº£nh: {str(img_error)}"]
                    yield "", chat_history
            else:
                chat_history.append([message, bot_message])
                yield "", chat_history
        else:
            chat_history.append([message, "KhÃ´ng cÃ³ pháº£n há»“i"])
            yield "", chat_history

    except Exception as e:
        print(f"Error: {str(e)}")
        error_message = f"Lá»—i: {str(e)}. Vui lÃ²ng kiá»ƒm tra LM Studio Ä‘ang cháº¡y táº¡i http://127.0.0.1:1234"
        chat_history.append([message, error_message])
        yield "", chat_history

    return "", chat_history

# ============================================================
# Khá»Ÿi táº¡o Gradio Interface
# ============================================================
with gr.Blocks() as demo:
    gr.Markdown("# ğŸ¤– Chatbot AI + Image Generator")
    gr.Markdown("Sá»­ dá»¥ng LM Studio (Gemma-3n) Ä‘á»ƒ chat")
    gr.Markdown("âš ï¸ **LÆ°u Ã½**: HÃ£y cháº¯c cháº¯n LM Studio Ä‘ang cháº¡y táº¡i `http://127.0.0.1:1234`")
    gr.Markdown("ğŸ’¡ **CÃ¡ch dÃ¹ng**: HÃ£y yÃªu cáº§u bot váº½ áº£nh, vÃ­ dá»¥: 'Váº½ má»™t con mÃ¨o Ä‘Ã¡ng yÃªu' hoáº·c 'Draw a cat'")
    
    message = gr.Textbox(label="Nháº­p tin nháº¯n cá»§a báº¡n:")
    chatbot = gr.Chatbot(label="Chat Bot siÃªu thÃ´ng minh", height=600)
    message.submit(chat_logic, [message, chatbot], [message, chatbot])

if __name__ == "__main__":
    print("\n" + "="*60)
    print("ğŸš€ Khá»Ÿi Ä‘á»™ng Chatbot...")
    print("="*60)
    print("âœ… Káº¿t ná»‘i LM Studio: http://127.0.0.1:1234")
    print("âœ… Model: Gemma-3n (e4b)")
    print("âœ… Há»— trá»£ image generation")
    print("="*60 + "\n")
    
    demo.launch()
